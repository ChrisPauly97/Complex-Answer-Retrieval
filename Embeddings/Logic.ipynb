{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/leftclick/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imports complete\n",
      "Functions Imported\n"
     ]
    }
   ],
   "source": [
    "%run Imports.ipynb\n",
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a query Id in the form Aftertaste/Distinguishing%20aftertaste%20and%20flavor\n",
    "# Returns aftertaste distinguishing aftertaste flavor\n",
    "# Removes slashes, %20, and lower cases, as well as removing stopwords.\n",
    "def formatQuery(queryId,stem):\n",
    "    queryIdWords = [x.lower() for x in queryId.replace(\"/\",\" \").replace(\"%20\",\" \").split(\" \") if x.lower() not in stopWords]\n",
    "    if(stem == True):\n",
    "        return ' '.join(set(stemmer.stem(word) for word in queryIdWords)).strip()\n",
    "    else:\n",
    "        return ' '.join(list(dict.fromkeys(word for word in queryIdWords))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes the paragraph associated with a query\n",
    "# Returns the unique words without stopwords\n",
    "def formatQueryParagraph(queryText,stem):\n",
    "    queryParagraph = [x.strip() for x in queryText.split(\" \") if x not in stopWords]\n",
    "    if(stem):\n",
    "        return ' '.join(set(stemmer.stem(word) for word in queryParagraph))\n",
    "    return ' '.join(set(word for word in queryParagraph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testParagraphFile = \"../Galago/data/paragraphCorpus.txt\"\n",
    "# sdmQueryFile = \"../Galago/Queries/sdm/rih-query.json\"\n",
    "\n",
    "# QrelMap is a map of qrel ID to \n",
    "def loadData (qrelMap, qrelTestMap,queryText,splitLine):\n",
    "#     qrelText = \"../Java_API/Car-Grpc/src/main/resources/data/General/qrelMap-test.txt\"\n",
    "    qrelText = \"../Java_API/Car-Grpc/src/main/resources/data/qrelMap-train.txt\"\n",
    "\n",
    "#     queryFile = \"../Java_API/Car-Grpc/src/main/resources/Queries/Glove/rih-0-combine-query.json\"\n",
    "    paragraphFile = \"paragraphCorpus.txt\"\n",
    "    \n",
    "    with open(paragraphFile,'r') as f:\n",
    "        for line in f:\n",
    "            splitLine.append(list(filter(None,' '.join(line.split(\"__PERIOD__\")).split(\" \"))))\n",
    "\n",
    "#     with open(queryFile,'r') as f:\n",
    "#         p = re.compile(\"\\((.*?) \\)\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             queryText += ' '.join([x.strip() for x in p.findall(line)])\n",
    "\n",
    "    with open(qrelText,'r') as f:\n",
    "        for line in f:\n",
    "            test = line.split(\":\")\n",
    "            \n",
    "            key = formatQuery(test[1],False).strip()\n",
    "    \n",
    "            value = formatQueryParagraph(test[2],False).strip()\n",
    "            if(key not in qrelMap):\n",
    "                qrelMap[key] = [value]\n",
    "            else:\n",
    "                qrelMap[key].append(value)\n",
    "#     with open(qrelTestText,'r') as f:\n",
    "#         for line in f:\n",
    "#             test = line.split(\":\")\n",
    "            \n",
    "#             key = formatQuery(test[1],False).strip()\n",
    "    \n",
    "#             value = formatQueryParagraph(test[2],False).strip()\n",
    "#             if(key not in qrelTestMap):\n",
    "#                 qrelTestMap[key] = [value]\n",
    "#             else:\n",
    "#                 qrelTestMap[key].append(value)   \n",
    "\n",
    "    print(\"loading complete\")\n",
    "    return queryText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete\n"
     ]
    }
   ],
   "source": [
    "qrelMap = dict()\n",
    "qrelTestMap = dict()\n",
    "queryText = \"\"\n",
    "splitLine = []\n",
    "queryText = loadData(qrelMap,qrelTestMap,queryText,splitLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the weighted average embedding based on tf-idf calculation\n",
    "# Gives a better representation of the average embedding\n",
    "def tfidfCalc(testQuery,paragraphs):\n",
    "    test = 0\n",
    "    embeddings = []\n",
    "    embeddingVocab = glove_model.wv.vocab\n",
    "#     formattedQuery = formatQuery(testQuery,False)\n",
    "    tfMap = {word: 0 for word in testQuery.split(\" \")}\n",
    "    dfMap = {word: 0 for word in testQuery.split(\" \")}\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        for word in testQuery.split(\" \"):\n",
    "            if(word in paragraph):\n",
    "                dfMap[word] += 1\n",
    "                tfMap[word] += paragraph.count(word)\n",
    "    for key in dfMap:\n",
    "        tfMap[key] = tfMap[key]/len(paragraphs)\n",
    "        if(dfMap[key] == 0):\n",
    "            dfMap[key] = 0.001\n",
    "        dfMap[key] = np.log(len(paragraphs)/dfMap[key])\n",
    "        test = tfMap[key] * dfMap[key]\n",
    "        if key in embeddingVocab:\n",
    "            embeddings.append(glove_model[key] * test)\n",
    "    \n",
    "    averageEmbedding = np.sum(embeddings,axis = 0)\n",
    "    return averageEmbedding\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create map of word to top n simiar words.\n",
    "# This uses the embedding of each word to find the most similar words\n",
    "# Returns a string mapping the top n words related to each word in the query.\n",
    "# Removes any words that exist in the original query\n",
    "def independentN(queryWords,stem,n):\n",
    "    independentSimilar = \"\"\n",
    "    independentSimilarList = []\n",
    "    queryWords = queryWords.split(\" \")\n",
    "#     print(queryWords)\n",
    "    vocab = glove_model.wv.vocab\n",
    "#     print([word for word in queryWords])\n",
    "#     print([word in vocab for word in queryWords])\n",
    "    similarMap = [(word,glove_model.most_similar(positive=[word],topn=n)) for word in queryWords if word in vocab]\n",
    "    for key,value in similarMap:\n",
    "        if(key in vocab and re.sub('[^a-zA-Z]+',\"\",key) != \"\"):\n",
    "            if(stem):\n",
    "                independentSimilar = re.sub('[^a-zA-Z]+',\"\",key) +':' + ' '.join(set(stemmer.stem(x[0]) for x in value if x[0] not in queryWords))\n",
    "            else:\n",
    "                independentSimilar = re.sub('[^a-zA-Z]+',\"\",key) +':' + ' '.join([re.sub('[^a-zA-Z]+',\"\",x[0])+ \"+\" + str(round(x[1],4)) for x in value if x[0] not in queryWords and re.sub('[^a-zA-Z]+',\"\",x[0]) != \"\"])\n",
    "            independentSimilarList.append(independentSimilar)\n",
    "        else:\n",
    "            continue\n",
    "#     print(independentSimilarList)\n",
    "    return independentSimilarList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independentN(testQuery,False,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare average embedding to each retrieved word \n",
    "# Takes a queryId\n",
    "# This is very rough, as it takes the average of the entire topN\n",
    "def independentRelevance(queryId):\n",
    "    relevantExpansions = []\n",
    "    relevantExpansionWords = \"\"\n",
    "    queryEmbedding = tfidfCalc(queryId,qrelMap[queryId])\n",
    "\n",
    "    independentEmbedding = independentN(queryId,False,10)\n",
    "    for iE in independentEmbedding:\n",
    "        independentEmbedding = averageEmbedding(' '.join(set(iE.split(\":\")[1].split(\" \"))))\n",
    "#         print(iE.split(\":\")[0])\n",
    "#         print(abs(np.sum(queryEmbedding - independentEmbedding)))\n",
    "        if(abs(np.sum(queryEmbedding - independentEmbedding)) < 10):\n",
    "            relevantExpansions.append(iE)\n",
    "            relevantExpansionWords += (iE.split(\":\")[0]) + \" \"\n",
    "    return [relevantExpansionWords.strip(),relevantExpansions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average embedding value of a query\n",
    "# Takes a query\n",
    "def averageEmbedding(queryId):\n",
    "    embeddings = []\n",
    "    queryList =queryId.split(\" \")\n",
    "    embeddingVocab = glove_model.wv.vocab\n",
    "    for queryWord in queryList:\n",
    "        if queryWord in embeddingVocab:\n",
    "            embeddings.append(glove_model[queryWord.strip()])\n",
    "        averageEmbedding = np.sum(embeddings,axis = 0)\n",
    "    return averageEmbedding\n",
    "\n",
    "\n",
    "\n",
    "# Function to create list of the top N similar words\n",
    "# This uses the average embedding of the entire query\n",
    "# Returns a list of the top n words, removing any words that exist in the original query.\n",
    "def averageN(queryId,stem, n):\n",
    "    avgE = tfidfCalc(queryId,qrelMap[queryId])\n",
    "    if(avgE.size == 300):\n",
    "        if(stem):\n",
    "            return ' '.join(queryId.split(\" \")) + ':' + ' '.join(set(stemmer.stem(x[0]) for x in glove_model.similar_by_vector(avgE,topn=n) if x[0] not in queryId))\n",
    "        return ' '.join(queryId.split(\" \")) + ':' + ' '.join(set(x[0] for x in glove_model.similar_by_vector(avgE,topn=n) if x[0] not in queryId))\n",
    "\n",
    "\n",
    "\n",
    "# Function to create list of the top N similar words and their similarity measures\n",
    "# This uses the average embedding of the entire query\n",
    "# Returns a list of the top n words, removing any words that exist in the original query.\n",
    "def averageNScores(queryWord,stem, n,k):\n",
    "    queryExpansionWords =[]\n",
    "    queryWordEmbedding = glove_model[queryWord]\n",
    "    similarWords = glove_model.similar_by_vector(queryWordEmbedding,topn=n)\n",
    "    similarWords = [list(x) for x in similarWords if x[0] != queryWord]\n",
    "    \n",
    "    for i in range(5):\n",
    "        mostSimilar = similarWords.pop(0)\n",
    "        queryExpansionWords.append(mostSimilar)\n",
    "        for x in similarWords:\n",
    "            x[1] = glove_model.similarity(x[0],mostSimilar[0]) \n",
    "        similarWords = sorted(similarWords, key=lambda x: x[1],reverse=True)\n",
    "#         print(\"SORTED\" + str(similarWords))\n",
    "        similarWords = similarWords[:len(similarWords)-k]\n",
    "    return queryExpansionWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query word, check each independent embedding word\n",
    "# If the embedding word is not relevant enough to the average of the query embedding disregard it.\n",
    "# Return a string containing all relevant words for each query word\n",
    "# Filters based on the average query embedding using tfidf\n",
    "def independentRelevanceFine(queryId,paragraphs):\n",
    "    relevantExpansions = \"\"\n",
    "    queryEmbedding = tfidfCalc(queryId,paragraphs)\n",
    "    independentEmbedding = independentN(queryId,False,10)\n",
    "    outputEmbeddings = dict()\n",
    "    for iE in independentEmbedding:\n",
    "        queryWord = iE.split(\":\")[0]\n",
    "        independentEList = iE.split(\":\")[1].split(\" \")\n",
    "        relevantExpansionWords = []\n",
    "        for word in independentEList:\n",
    "            if word in glove_model.wv.vocab:\n",
    "                wordEmbedding = glove_model[word]\n",
    "                if(abs(np.sum(queryEmbedding - wordEmbedding))<15):\n",
    "                    relevantExpansionWords.append(word + \" \")\n",
    "        if(relevantExpansionWords != []):\n",
    "            outputEmbeddings[queryWord] = relevantExpansionWords\n",
    "            \n",
    "    return outputEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query word, check each independent embedding word\n",
    "# If the embedding word is not relevant enough to the average of the query embedding disregard it.\n",
    "# Return a string containing all relevant words for each query word\n",
    "# Filters based on the average query embedding using tfidf\n",
    "def independentRelevanceN(queryId,paragraphs,n):\n",
    "    relevantExpansions = \"\"\n",
    "    queryEmbedding = tfidfCalc(queryId,paragraphs)\n",
    "    independentEmbedding = independentN(queryId, False, n)\n",
    "    outputEmbeddings = \"\"\n",
    "    \n",
    "    for iE in independentEmbedding:\n",
    "        queryWord = iE.split(\":\")[0]\n",
    "        independentEList = iE.split(\":\")[1].split(\" \")\n",
    "        relevantExpansionWords = \" \"\n",
    "        for word in independentEList:\n",
    "            if word in glove_model.wv.vocab:\n",
    "\n",
    "                wordEmbedding = glove_model[word]\n",
    "                if(abs(np.sum(queryEmbedding - wordEmbedding))<25):\n",
    "                    relevantExpansionWords += word + \" \"\n",
    "        \n",
    "        if(relevantExpansionWords != []):\n",
    "            relevantExpansionWords = ' '.join(sorted(relevantExpansionWords.split(\" \")))\n",
    "            outputEmbeddings += queryWord + \":\" + relevantExpansionWords + \";\"\n",
    "#         else:\n",
    "#             print(word)\n",
    "            \n",
    "    return outputEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which checks the ratio of top n similar words that actually appear\n",
    "# in the paragraph associated with the query in the qrel file.\n",
    "# Takes the paragraphs and the queryId and the number of similar words to search for\n",
    "def matchedAverage(qrelParas, queryId,n):\n",
    "    for paragraph in set(qrelParas):\n",
    "        matched = averageN(queryId,False,n).split(\":\")[1].split(\" \")\n",
    "        matchedWords = set([x for x in matched if x in paragraph])\n",
    "        if(matchedWords == set()):\n",
    "            return [\"\",0]\n",
    "        else:\n",
    "            return [list(matchedWords),len(matchedWords)/len(matched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which checks the ratio of top n similar words that actually appear\n",
    "# in the paragraph associated with the query in the qrel file.\n",
    "# Takes the paragraphs and the queryId and the number of similar words to search for\n",
    "def matchedIndependent(qrelParas,queryId,n):\n",
    "    \n",
    "    allMatched = []\n",
    "    for paragraph in set(qrelParas):\n",
    "        matchedList = independentN(queryId,False,n)\n",
    "        for word in matchedList:\n",
    "            matched = word.split(\":\")[1].split(\" \")\n",
    "            matchedWords = set(x for x in matched if x in paragraph)\n",
    "            if(matchedWords != set()):\n",
    "                allMatched.append(list(matchedWords))\n",
    "    flatList = set(x for y in allMatched for x in y)\n",
    "\n",
    "    return [list(flatList),len(flatList)/len(matched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allergy:allergies+0.7246 allergic+0.6522 asthma+0.6489 fauci+0.5374 sufferers+0.5139 arthritis+0.5057 allergens+0.5012 medication+0.4827 medications+0.4824 anaphylaxis+0.4758',\n",
       " 'cause:causes+0.8481 caused+0.7603 causing+0.7388 damage+0.595 serious+0.5714 severe+0.5707 result+0.5643 harm+0.5536 might+0.5496 likely+0.5461',\n",
       " 'environmental:ecological+0.6484 pollution+0.6385 conservation+0.6369 environment+0.6296 protection+0.5658 environmentalists+0.5566 epa+0.5496 safety+0.5473 sustainability+0.5444 climate+0.5329',\n",
       " 'factors:factor+0.7534 determining+0.5523 affect+0.5515 reasons+0.5485 considerations+0.5426 mitigating+0.536 impact+0.5131 particular+0.5085 increases+0.5082 affecting+0.5037']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testQuery = formatQuery(\"Allergy/Cause/Other%20environmental%20factors \",False)\n",
    "\n",
    "testQueryList = list(islice(qrelMap, 5))\n",
    "\n",
    "independentN(testQuery,False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.035s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestStringMethods(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        cls.testString = \"immigration federal agriprocessors raid controversies:enforcement investigations illegal laws fbi investigation\"\n",
    "        cls.testQuery = \"agriprocessors controversies federal immigration raid\"\n",
    "        \n",
    "    def test_format_query(self):\n",
    "        self.assertEqual(sorted(formatQuery(\"Agriprocessors/Controversies/Federal%20immigration%20raid\",False).split(\" \")),sorted(self.testQuery.split(\" \")))\n",
    "\n",
    "    def test_average_embedding(self):\n",
    "        self.assertEqual(len(tfidfCalc(self.testQuery,qrelMap[self.testQuery])), glove_model.vector_size)\n",
    "\n",
    "    def test_average_embedding_length(self):\n",
    "        self.assertTrue(averageN(testQuery,False,10),self.testString)\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out independent embedding map\n",
      "Independent embedding map written to topIN.txt\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing out independent embedding map\")\n",
    "topANarr = []\n",
    "allWords = []\n",
    "with open(\"../Java_API/Car-Grpc/src/main/resources/data/topIN/independent-Glove-train.txt\",'w') as topIN:\n",
    "    topINarr = []\n",
    "    for x in qrelMap:        \n",
    "        uniqueQueryWords = formatQuery(x,False)\n",
    "        allWords.extend([x for x in uniqueQueryWords.split(\" \") if x not in allWords and x.lower() not in stopWords])\n",
    "    for word in allWords:\n",
    "        expansionTerms = independentN(word, False, 1000)\n",
    "        if(expansionTerms != []):\n",
    "            topINarr.extend(expansionTerms)\n",
    "    topIN.write(';'.join(topINarr))\n",
    "    print(\"Independent embedding map written to topIN.txt\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing out independent embedding map\")\n",
    "topANarr = []\n",
    "with open(\"../Java_API/Car-Grpc/src/main/resources/data/StopWords\") as sWordsFile:\n",
    "    stopWords = sWordsFile.read().split(\"\\n\")\n",
    "allWords = []\n",
    "with open(\"../Java_API/Car-Grpc/src/main/resources/data/topIN/topIN-test.txt\",'w') as topIN:\n",
    "    topINarr = []\n",
    "    for x in qrelMap:        \n",
    "        uniqueQueryWords = formatQuery(x,False)\n",
    "        allWords.extend([x for x in uniqueQueryWords.split(\" \") if x not in allWords and x not in stopWords])\n",
    "    for word in allWords:\n",
    "        expansion = independentN(word, False, 100)\n",
    "        words = []\n",
    "        if(expansion != []):\n",
    "            e = expansion[0].split(\":\")[1].split(\" \")\n",
    "            for w in e:\n",
    "                if(len(w) > 1 and w not in stopWords):\n",
    "#                 if w in ' '.join(qrelMap[uniqueQueryWords]).split(\" \") and len(w) > 1 and w not in stopWords: \n",
    "                    words.append(w)\n",
    "            if words != []:\n",
    "                output = [expansion[0].split(\":\")[0] +\":\" + ' '.join(words)]\n",
    "                topINarr.extend(output)\n",
    "    topIN.write(';'.join(topINarr))\n",
    "    print(\"Independent embedding map written to topN.txt\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetQueriesList = []\n",
    "with open(\"test-random-subset.txt\") as subsetQueries:\n",
    "    for line in subsetQueries:\n",
    "        subsetQueriesList.append(line.replace(\"\\n\",\"\"))\n",
    "#     while (line = subsetQueries.readline()) != null:\n",
    "#         print(subsetQueries.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Average ELMo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enwiki:Ionic%20compound/Structure/Defects', 'enwiki:Antibiotics/Medical%20uses/Administration', 'enwiki:Instant%20coffee/Use', 'enwiki:Beach%20nourishment/Causes%20of%20erosion', 'enwiki:Coevolution/Coevolution%20outside%20biology', 'enwiki:Storm%20surge/Historic%20storm%20surges', 'enwiki:Disturbance%20(ecology)/Importance', 'enwiki:Anti-Slavery%20International/Overview', 'enwiki:Disgust/Morality/Political%20orientation', 'enwiki:World%20Health%20Organization/Controversies', 'enwiki:Onboarding/Recommendations%20for%20practitioners', 'enwiki:Drinking%20water/Regulation/European%20Union', 'enwiki:Ionic%20compound/Formation', 'enwiki:Mexican%20cuisine/Regional%20cuisines/Northern%20Mexico', 'enwiki:Purified%20water/Health%20effects%20of%20drinking%20purified%20water', 'enwiki:Elaeis%20guineensis/Disease', 'enwiki:Tsunami/Generation%20mechanisms/Man-made%20or%20triggered%20tsunamis', 'enwiki:Non-governmental%20organization/Critiques/Challenges%20to%20legitimacy', 'enwiki:Instant%20coffee/Health%20hazards', 'enwiki:Olfaction/Main%20olfactory%20system/Receptor%20neuron', 'enwiki:Olive%20oil/Regulation/Commercial%20grades', 'enwiki:Cocoa%20butter/Extraction%20and%20composition', 'enwiki:Dehumidifier/Applications', 'enwiki:Capsaicin/Biosynthesis/Biosynthetic%20Pathway', 'enwiki:Geomagnetic%20reversal/Geomagnetic%20polarity%20time%20scale/Superchrons', 'enwiki:Non-governmental%20organization/Influence%20of%20NGOs%20upon%20world%20affairs', 'enwiki:Millennium%20Development%20Goals/Related%20activities/organisations/UN%20Goals', 'enwiki:Purified%20water/Purification%20methods/Demineralization', 'enwiki:Deforestation/Public%20health%20context', 'enwiki:Wetland/Hydrology/Biota', 'enwiki:History%20of%20catecholamine%20research/Membrane%20passage', 'enwiki:Precipitation/Formation/Hail', 'enwiki:Water%20fluoridation%20controversy/Evidence/Safety', 'enwiki:Malolactic%20fermentation/Wine%20faults/Mousiness%20and%20geranium%20taint', 'enwiki:Malolactic%20fermentation/Wine%20faults/Fresno%20mold%20and%20ropiness', 'enwiki:Permaculture/Theory/Zones', 'enwiki:Non-governmental%20organization/Corporate%20structure/Monitoring%20and%20control', 'enwiki:Urban%20sprawl/Definition%20&%20Characteristics/Single-use%20development', 'enwiki:Hot%20chocolate/Health/Risks', 'enwiki:Insular%20cortex/Function/Interoceptive%20awareness', 'enwiki:Wetland/Climates/Rainfall', 'enwiki:Division%20of%20labour/Globalization%20and%20global%20division%20of%20labour', 'enwiki:Millennium%20Development%20Goals/Progress/Review%20Summit%202010', 'enwiki:Human%20trafficking/Overview', 'enwiki:Supercapacitor/Producers', 'enwiki:Hybrid%20electric%20vehicle/Legislation%20and%20incentives/United%20States', 'enwiki:Perception/Theories/Perception%20as%20direct%20perception', 'enwiki:Malolactic%20fermentation/Preventing%20MLF', 'enwiki:Insular%20cortex/Clinical%20significance/Subjective%20certainty%20in%20ecstatic%20seizures', 'enwiki:Millennium%20Development%20Goals/Improvements']\n"
     ]
    }
   ],
   "source": [
    "# print(subsetQueriesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compound', 'defects', 'ionic', 'structure']\n",
      "['administration', 'medical', 'uses', 'antibiotics']\n",
      "['coffee', 'instant', 'use']\n",
      "['beach', 'nourishment', 'erosion', 'causes']\n",
      "['biology', 'coevolution', 'outside']\n",
      "['storm', 'surges', 'historic', 'surge']\n",
      "['(ecology)', 'disturbance', 'importance']\n",
      "['overview', 'international', 'anti-slavery']\n",
      "['orientation', 'disgust', 'morality', 'political']\n",
      "['world', 'health', 'controversies', 'organization']\n",
      "['practitioners', 'onboarding', 'recommendations']\n",
      "['european', 'water', 'union', 'drinking', 'regulation']\n",
      "['compound', 'formation', 'ionic']\n",
      "['regional', 'cuisine', 'mexican', 'cuisines', 'northern', 'mexico']\n",
      "['purified', 'water', 'drinking', 'health', 'effects']\n",
      "['elaeis', 'disease', 'guineensis']\n",
      "['generation', 'tsunami', 'tsunamis', 'mechanisms', 'man-made', 'triggered']\n",
      "['non-governmental', 'challenges', 'organization', 'critiques', 'legitimacy']\n",
      "['hazards', 'coffee', 'instant', 'health']\n",
      "['olfactory', 'receptor', 'neuron', 'olfaction', 'system', 'main']\n",
      "['grades', 'commercial', 'olive', 'oil', 'regulation']\n",
      "['extraction', 'cocoa', 'butter', 'composition']\n",
      "['dehumidifier', 'applications']\n",
      "['pathway', 'biosynthesis', 'capsaicin', 'biosynthetic']\n",
      "['reversal', 'scale', 'time', 'superchrons', 'polarity', 'geomagnetic']\n",
      "['world', 'ngos', 'non-governmental', 'upon', 'affairs', 'organization', 'influence']\n",
      "['millennium', 'related', 'organisations', 'development', 'goals', 'un', 'activities']\n",
      "['purified', 'demineralization', 'methods', 'water', 'purification']\n",
      "['deforestation', 'health', 'public', 'context']\n",
      "['wetland', 'hydrology', 'biota']\n",
      "['catecholamine', 'membrane', 'passage', 'research', 'history']\n",
      "['precipitation', 'formation', 'hail']\n",
      "['controversy', 'evidence', 'water', 'fluoridation', 'safety']\n",
      "['fermentation', 'faults', 'wine', 'mousiness', 'taint', 'geranium', 'malolactic']\n",
      "['mold', 'fresno', 'fermentation', 'faults', 'wine', 'ropiness', 'malolactic']\n",
      "['zones', 'theory', 'permaculture']\n",
      "['control', 'monitoring', 'non-governmental', 'organization', 'structure', 'corporate']\n",
      "['development', 'single-use', 'characteristics', 'definition', '&', 'urban', 'sprawl']\n",
      "['risks', 'health', 'hot', 'chocolate']\n",
      "['function', 'interoceptive', 'cortex', 'insular', 'awareness']\n",
      "['wetland', 'climates', 'rainfall']\n",
      "['globalization', 'labour', 'division', 'global']\n",
      "['millennium', 'progress', 'development', 'goals', 'review', 'summit', '2010']\n",
      "['trafficking', 'overview', 'human']\n",
      "['producers', 'supercapacitor']\n",
      "['electric', 'vehicle', 'incentives', 'united', 'hybrid', 'states', 'legislation']\n",
      "['direct', 'theories', 'perception']\n",
      "['mlf', 'fermentation', 'preventing', 'malolactic']\n",
      "['clinical', 'ecstatic', 'cortex', 'insular', 'seizures', 'certainty', 'subjective', 'significance']\n",
      "['development', 'goals', 'millennium', 'improvements']\n",
      "112.84513759613037\n",
      "85.98184251785278\n",
      "86.08951115608215\n",
      "94.49556016921997\n",
      "127.56638693809509\n",
      "175.55606770515442\n",
      "124.85508465766907\n",
      "128.2139413356781\n",
      "120.0066409111023\n",
      "90.73514008522034\n",
      "79.2406439781189\n",
      "114.7254114151001\n",
      "124.09810256958008\n",
      "112.48545002937317\n",
      "178.37158274650574\n",
      "122.16832160949707\n",
      "108.37623119354248\n",
      "116.95962047576904\n",
      "119.71579170227051\n",
      "96.26540946960449\n",
      "88.0948429107666\n",
      "96.83598327636719\n",
      "96.25357389450073\n",
      "102.53308081626892\n",
      "104.29070925712585\n",
      "89.2625298500061\n",
      "85.76430010795593\n",
      "88.80752754211426\n",
      "84.30561852455139\n",
      "83.76657056808472\n",
      "82.74638557434082\n",
      "87.51451110839844\n",
      "82.74692010879517\n",
      "75.46601176261902\n",
      "75.81990170478821\n",
      "75.52646350860596\n",
      "77.80535936355591\n",
      "78.63320517539978\n",
      "79.63176369667053\n",
      "79.74797201156616\n",
      "78.05377745628357\n",
      "76.38878583908081\n",
      "76.47734498977661\n",
      "76.4054319858551\n",
      "76.13542318344116\n",
      "76.52489185333252\n",
      "76.23688817024231\n",
      "76.22991919517517\n",
      "76.080087184906\n",
      "76.78698015213013\n"
     ]
    }
   ],
   "source": [
    "h5py_file = h5py.File(\"elmo-embeddings/elmo-embeddings-global-avg.hdf5\", 'r')\n",
    "avg_queries = []\n",
    "\n",
    "with open(\"500kvocab.txt\",\"r\") as vocab_file: # Get all Vocab words and link this to the elmo embeddings\n",
    "    vocab_list = vocab_file.read().split(\"\\n\")\n",
    "    vocab_to_elmo = dict()\n",
    "\n",
    "    for x in range(0,len(vocab_list)-1):     # For every word in the vocab, create the key value pair.\n",
    "        vocab_to_elmo[vocab_list[x]] = h5py_file.get(str(x))\n",
    "        \n",
    "for q in subsetQueriesList:\n",
    "    tokens = list(set(formatQuery(q, False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "    print(tokens)\n",
    "    avg_tokens = []\n",
    "    vectors = elmo.embed_sentence(tokens)\n",
    "    avg_query = np.average(np.average(vectors,axis=0),axis=0)\n",
    "    avg_queries.append(avg_query)\n",
    "\n",
    "average_similarities = defaultdict(list)\n",
    "y = 0\n",
    "for averageQ in avg_queries:\n",
    "    start = time.time()\n",
    "    similarities = dict()\n",
    "    for x in vocab_to_elmo:     # For every vocab word\n",
    "        test = scipy.spatial.distance.cosine(vocab_to_elmo[x],averageQ)        # Store the cosine similarity to the average query in a submap\n",
    "        similarities[x] = test\n",
    "    average_similarities[subsetQueriesList[y]].append(similarities)    # Store the word and its similarity in a map of query to similarity.\n",
    "    end = time.time() - start\n",
    "    print(end)\n",
    "\n",
    "#     print(end)\n",
    "    y += 1\n",
    "    \n",
    "#Similarities is a map of vocab word to similarity to the average query.\n",
    "# List with entry of all most similar words for each query word.\n",
    "\n",
    "import operator\n",
    "import re\n",
    "with open(\"Global_ELMo/global_avg_similarity.txt\",\"w\") as f:\n",
    "    for x in range(0,len(subsetQueriesList)):\n",
    "        tokens = list(set(formatQuery(subsetQueriesList[x], False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "        sorted_d = sorted(average_similarities[subsetQueriesList[x]][0].items(), key=operator.itemgetter(1))\n",
    "        f.write(str(subsetQueriesList[x]).replace(\"\\n\",\"\") + \"@\" + str([x[0] +\"+\" + str(x[1]) for x in sorted_d[:150] if re.sub('[^a-zA-Z]+',\"\",x[0]) not in tokens]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bbc449f26ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsetQueriesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsetQueriesList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enwiki:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msorted_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_similarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubsetQueriesList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsetQueriesList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"@\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"+\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z]+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average_similarities' is not defined"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import re\n",
    "with open(\"Global_ELMo/global_avg_similarity.txt\",\"w\") as f:\n",
    "    for x in range(0,len(subsetQueriesList)):\n",
    "        tokens = list(set(formatQuery(subsetQueriesList[x], False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "        sorted_d = sorted(average_similarities[subsetQueriesList[x]][0].items(), key=operator.itemgetter(1))\n",
    "        f.write(str(subsetQueriesList[x]).replace(\"\\n\",\"\") + \"@\" + str([x[0] +\"+\" + str(x[1]) for x in sorted_d[:1000] if re.sub('[^a-zA-Z]+',\"\",x[0]) not in tokens]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load independent and dependent local ELMo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context dependent Local ELMo paragraphs and queries - from run file\n",
    "# with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/train-subset-localELMo.json\", \"r\") as read_file:\n",
    "#     data = json.load(read_file)\n",
    "import json\n",
    "#Context dependent Local ELMo paragraphs and queries - from run file\n",
    "# with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/test-subset-localELMo.json\", \"r\") as read_file:\n",
    "#     data = json.load(read_file)\n",
    "    \n",
    "#Context independent Local ELMo paragraphs and queries - from run file\n",
    "with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/train-subset-indy-localELMo.json\", \"r\") as read_file:\n",
    "    indy_data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates query-vocab map for context independent Local ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context independent vocab of queries to list of most similar words with rankings, Local ELMo\n",
    "\n",
    "indy_query_vocab_map = dict()\n",
    "for query_num in range(50):\n",
    "    vocab_set = set()\n",
    "    for paragraph in indy_data.get(\"queries\")[query_num].get(\"Paragraphs\")[:20]:\n",
    "        for w in paragraph.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\"):\n",
    "            vocab_set.add(w)\n",
    "    indy_query_vocab_map[indy_data.get(\"queries\")[query_num].get(\"QueryId\")] = vocab_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates word-similarity map for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.13508200645447\n",
      "80.12757992744446\n",
      "63.86967325210571\n",
      "64.15473318099976\n",
      "49.01329731941223\n",
      "62.67295455932617\n",
      "63.50104594230652\n",
      "54.21759653091431\n",
      "63.258044719696045\n",
      "68.97375059127808\n",
      "55.325474977493286\n",
      "60.50427174568176\n",
      "66.39211392402649\n",
      "66.01664543151855\n",
      "53.547096252441406\n",
      "90.77966356277466\n",
      "74.03459286689758\n",
      "74.56365966796875\n",
      "80.88692235946655\n",
      "43.76298379898071\n",
      "55.44410943984985\n",
      "58.158263206481934\n",
      "54.69401812553406\n",
      "79.668710231781\n",
      "63.40876078605652\n",
      "72.80970788002014\n",
      "74.92783665657043\n",
      "79.57811617851257\n",
      "76.46160531044006\n",
      "54.99807000160217\n",
      "61.25098514556885\n",
      "88.28884816169739\n",
      "84.49814462661743\n",
      "56.924975872039795\n",
      "66.39566206932068\n",
      "78.55631828308105\n",
      "83.72816276550293\n",
      "95.51873350143433\n",
      "61.53771209716797\n",
      "70.40596842765808\n",
      "66.3760027885437\n",
      "89.37235474586487\n",
      "70.33120965957642\n",
      "74.26629900932312\n",
      "71.4531500339508\n",
      "72.5851879119873\n",
      "69.4743435382843\n",
      "44.398423194885254\n",
      "53.65299367904663\n",
      "81.54960703849792\n"
     ]
    }
   ],
   "source": [
    "# Context independent Word-Similarity map for all queries in the train set \n",
    "import time\n",
    "query_to_word_map = dict()\n",
    "\n",
    "for query in indy_data.get(\"queries\"): # For each query in the train set\n",
    "    querySimilarityList = []\n",
    "    querySimilarityMapList = []\n",
    "    sortedParagraphs = []\n",
    "    start = time.time()\n",
    "    \n",
    "    QueryId = query.get(\"QueryId\")\n",
    "    \n",
    "    # Get the formatted Query Id\n",
    "    queryFormatted = list(set(formatQuery(QueryId,False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "    \n",
    "    # Get the average embedding for each word in the query.\n",
    "    averageQueryEmbedding = np.average(elmo.embed_sentence(queryFormatted),axis=0)\n",
    "#     print(averageQueryEmbedding.shape)\n",
    "\n",
    "    # Average embedding of the entire query\n",
    "    entireQueryEmbedding = np.average(averageQueryEmbedding,axis=0)\n",
    "#     print(entireQueryEmbedding.shape)\n",
    "    \n",
    "    paragraphSimilarity = []\n",
    "    word_to_similarity_map = dict()\n",
    "    for word in indy_query_vocab_map[QueryId]: # For each word returned for that query\n",
    "        averageWordEmbedding = np.average(elmo.embed_sentence([word.strip()]),axis=0)\n",
    "        similarity = scipy.spatial.distance.cosine(entireQueryEmbedding,averageWordEmbedding) #Get the similarity\n",
    "        word_to_similarity_map[word] = similarity\n",
    "    query_to_word_map[QueryId] = word_to_similarity_map\n",
    "    end = time.time() - start\n",
    "    print(end)\n",
    "# print(query_to_word_map)\n",
    "\n",
    "#     import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File output for context independent Local ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out context independent weights to file for local ELMo\n",
    "\n",
    "import operator\n",
    "with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/noContextWeightsTest.txt\", \"w\") as f:\n",
    "    for q in query_to_word_map: # For each query word store the list, sorted the top n by that query word\n",
    "\n",
    "        sorted_x = sorted(query_to_word_map[q].items(), key=operator.itemgetter(1))\n",
    "\n",
    "        f.write(q + \"@\" + str([re.sub('[^0-9a-zA-Z]+',\"\",x[0]) +\"+\"+ str(x[1]) for x in sorted_x if x[0] not in q][:10000]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Dependent Local ELMo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.130402088165283\n",
      "14.677674055099487\n",
      "11.640497922897339\n",
      "11.794345140457153\n",
      "13.075102806091309\n",
      "13.187154293060303\n",
      "11.370903491973877\n",
      "13.11812710762024\n",
      "12.502481460571289\n",
      "24.354890823364258\n",
      "14.55323314666748\n",
      "13.51351547241211\n",
      "13.202198028564453\n",
      "14.649916887283325\n",
      "12.728102922439575\n",
      "11.496967792510986\n",
      "11.198596239089966\n",
      "15.023398160934448\n",
      "11.994405508041382\n",
      "14.722891330718994\n",
      "13.716572523117065\n",
      "11.847083806991577\n",
      "8.657777309417725\n",
      "12.111258268356323\n",
      "12.858834981918335\n",
      "16.836432695388794\n",
      "16.407613039016724\n",
      "12.931836128234863\n",
      "15.86539912223816\n",
      "13.054662704467773\n",
      "11.70007610321045\n",
      "12.831973791122437\n",
      "12.676647663116455\n",
      "12.360377073287964\n",
      "12.368247509002686\n",
      "12.524710655212402\n",
      "15.410269021987915\n",
      "14.853639125823975\n",
      "12.124898910522461\n",
      "14.844342947006226\n",
      "12.67747974395752\n",
      "14.718013286590576\n",
      "13.601699829101562\n",
      "15.923472166061401\n",
      "11.284835577011108\n",
      "14.526308059692383\n",
      "13.78287935256958\n",
      "12.07865571975708\n",
      "15.51027536392212\n",
      "15.803512334823608\n"
     ]
    }
   ],
   "source": [
    "# This block generates a list of the top n words most similar to each query word.\n",
    "# This is done by calculating the similarity between each word in the top 1000 paragraphs and the query words.\n",
    "# This is stored in a large list of maps {'qWord1': 0.256, 'qWord2': 0.814, 'qWord3': 0.783, ...}\n",
    "# It is sorted by each of these query words and the top most similar words are taken and added to a separate list.\n",
    "# This forms the output which is a list of length q containing the top n expansion words where q is the number of \n",
    "# query terms.\n",
    "\n",
    "wa = True\n",
    "for query in data.get(\"queries\"): # For each query in the train set\n",
    "    querySimilarityList = []\n",
    "    querySimilarityMapList = []\n",
    "    sortedParagraphs = []\n",
    "    start = time.time()\n",
    "    \n",
    "    # Get the formatted Query Id\n",
    "    queryFormatted = list(set(formatQuery(query.get(\"QueryId\"),False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "    \n",
    "    # Get the average embedding for each word in the query.\n",
    "    averageQueryEmbedding = np.average(elmo.embed_sentence(queryFormatted),axis=0)\n",
    "#     print(averageQueryEmbedding.shape)\n",
    "    entireQueryEmbedding = np.average(averageQueryEmbedding,axis=0)\n",
    "    \n",
    "    paragraphEmbeddingList = []\n",
    "    paragraphSimilarity = []\n",
    "    \n",
    "    for paragraph in query.get(\"Paragraphs\")[:20]: # For each paragraph returned for that query\n",
    "        sentenceList = []\n",
    "        sentenceSimilarity = []\n",
    "        \n",
    "        paragraph = paragraph.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\") # format paragraph\n",
    "        sentences = list(filter(None,''.join(paragraph).split(\"fullstop\")))\n",
    "\n",
    "        for x in range(0,len(sentences)): # For each sentence, make it a list and add it back.\n",
    "            cleanedSentence = sentences[x].strip().split(\" \")\n",
    "            sentences[x] = cleanedSentence\n",
    "            \n",
    "        sentencesEmbeddings = elmo.embed_batch(sentences) # Run elmo on the list of sentences in a paragraph\n",
    "\n",
    "        z = 0\n",
    "        \n",
    "        for sentenceEmbedding in sentencesEmbeddings: #For each sentence in the paragraph\n",
    "            sentence = sentences[z]\n",
    "            y = 0\n",
    "\n",
    "            perWordEmbedding = np.average(sentenceEmbedding, axis=0) # Average out each word embedding\n",
    "\n",
    "            for wordEmbed in perWordEmbedding: # For word in sentence\n",
    "                paraWordSimilarity = dict()\n",
    "                querySimilarityMap = dict()\n",
    "                x = 0\n",
    "            \n",
    "                paraWordSimilarity[\"sentenceWord\"] = sentence[y] # Store a reference to the word\n",
    "                similarity = round(scipy.spatial.distance.cosine(wordEmbed,entireQueryEmbedding),3) #Get the similarity\n",
    "                paraWordSimilarity[\"entireQuery\"] = similarity\n",
    "\n",
    "                for qWordEmbed in averageQueryEmbedding: # For query word\n",
    "            \n",
    "                    similarity = round(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed),3) #Get the similarity\n",
    "            \n",
    "                    paraWordSimilarity[queryFormatted[x]] = similarity # Store the similarity mapped to query word\n",
    "                    x += 1\n",
    "                    \n",
    "                if(wa): # Weighted Average Flag\n",
    "                    if not any(d.get(\"sentenceWord\",None) == sentence[y] for d in querySimilarityMapList):\n",
    "                        x = 0\n",
    "                        querySimilarityMap[\"sentenceWord\"] = sentence[y] # Store a reference to the word\n",
    "                        similarity = round(scipy.spatial.distance.cosine(wordEmbed,entireQueryEmbedding),3) #Get the similarity\n",
    "                        querySimilarityMap[\"entireQuery\"] = similarity\n",
    "\n",
    "                        for qWordEmbed in averageQueryEmbedding:\n",
    "                            similarity = round(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed),3) #Get the similarity\n",
    "\n",
    "                            querySimilarityMap[queryFormatted[x]] = [similarity]\n",
    "                            x += 1\n",
    "                        querySimilarityMapList.append(querySimilarityMap)\n",
    "                    else:\n",
    "                        for m in range(0,len(querySimilarityMapList)-1):\n",
    "                            if querySimilarityMapList[m][\"sentenceWord\"] == sentence[y]:\n",
    "                                x = 0\n",
    "                                for qWordEmbed in averageQueryEmbedding:\n",
    "                                    similarity = round(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed),3) #Get the similarity\n",
    "                                    querySimilarityMapList[m].get(queryFormatted[x]).append(similarity)\n",
    "                                    x += 1\n",
    "                else:\n",
    "                    querySimilarityList.append(paraWordSimilarity) # Store all similarities for the query\n",
    "                y += 1\n",
    "            z += 1\n",
    "    \n",
    "    end_time = time.time() - start\n",
    "    print(end_time)\n",
    "    import re\n",
    "\n",
    "    \n",
    "    queryFormatted.append(\"entireQuery\")\n",
    "    for k in queryFormatted: # For each query word store the list, sorted the top n by that query word\n",
    "        testList = []\n",
    "\n",
    "        if(wa):\n",
    "            for d in querySimilarityMapList:\n",
    "#                 queryFormatted = queryFormatted.append(\"entireQuery\")\n",
    "                for q in queryFormatted:\n",
    "                    d[q] = round(np.average(d[q]),3) # Choose top N occurrences by sorting and slicing\n",
    "            testList = sorted(querySimilarityMapList, key=lambda x: (x[k]))\n",
    "#             print(re.sub('[^0-9a-zA-Z]+',\"\",testList[0].get(\"sentenceWord\")))\n",
    "            if(k == \":\"):\n",
    "                k == \"colon\"\n",
    "            if(k == \"entireQuery\"):\n",
    "                expansionWords = k + \":\" + ' '.join(set([re.sub('[^0-9a-zA-Z]+',\"\",x.get(\"sentenceWord\")) +\"+\"+ str(x.get(k)) for x in testList if x.get(\"sentenceWord\") != k]))\n",
    "            else:\n",
    "                expansionWords = k + \":\" + ' '.join(set([re.sub('[^0-9a-zA-Z]+',\"\",x.get(\"sentenceWord\")) +\"+\"+ str(x.get(k)) for x in testList if x.get(\"sentenceWord\") != k]))\n",
    "\n",
    "        else:\n",
    "            testList = sorted(querySimilarityList, key=lambda x: (x[k]))\n",
    "            if(k == \"entireQuery\"):\n",
    "                expansionWords = k + \":\" + ' '.join(set([re.sub('[^0-9a-zA-Z]+',\"\",x.get(\"sentenceWord\")) +\"+\"+ str(x.get(k)) for x in testList if x.get(\"sentenceWord\") != k]))\n",
    "            else:\n",
    "                expansionWords = k + \":\" + ' '.join(set([re.sub('[^0-9a-zA-Z]+',\"\",x.get(\"sentenceWord\")) +\"+\"+ str(x.get(k)) for x in testList if x.get(\"sentenceWord\") != k]))\n",
    "\n",
    "        sortedParagraphs.append(expansionWords)\n",
    "    if wa:\n",
    "        with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/embeddingsAverageWeightsUnlimitedTest.txt\", \"a\") as e:\n",
    "            e.write(query.get(\"QueryId\") + \"@\" + str(sortedParagraphs) + \"\\n\")\n",
    "    else:\n",
    "        with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/embeddingsGreedyWeights.txt\", \"a\") as e:\n",
    "            e.write(query.get(\"QueryId\") + \"@\" + str(sortedParagraphs) + \"\\n\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Code maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Java_API/Car-Grpc/src/main/resources/Queries/ELMo/test-random-subset.txt\",\"w\") as test_queries:\n",
    "    querySample = data.get(\"queries\")\n",
    "    sample50test = random.sample(querySample,50)\n",
    "    obj = json.dumps({'queries': sample50test})\n",
    "    json.dump(obj,test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_similarities = defaultdict(list)\n",
    "# y = 0\n",
    "# for averageQ in avg_queries[:1]:\n",
    "#     similarities = dict()\n",
    "#     # For every vocab word\n",
    "#     for x in vocab_to_elmo:\n",
    "#         # Store the cosine similarity to the average query in a submap\n",
    "#         test = scipy.spatial.distance.cosine(vocab_to_elmo[x],averageQ)\n",
    "#         similarities[x] = test\n",
    "#     # Store the word and its similarity in a map of query to similarity.\n",
    "#     average_similarities[subsetQueriesList[y]].append(similarities)\n",
    "#     y += 1\n",
    "\n",
    "# print(average_similarities[subsetQueriesList[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Global_ELMo/independentSimilarity.txt\",\"w\") as avgSim:\n",
    "#     for x in range(len(subsetQueriesList)):\n",
    "#         tokens = list(set(formatQuery(subsetQueriesList[x], False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "#         avgSim.write(subsetQueriesList[x] + \"@\" + str(sorted(average_similarities[x].items(), key=operator.itemgetter(1))) + \"\\n\")\n",
    "# import operator\n",
    "# y = 0\n",
    "# with open(\"Global_ELMo/independentSimilarity.txt\",\"w\") as avgSim:\n",
    "#     for y in range(len(subsetQueriesList) -1):\n",
    "#         for x in average_similarities[subsetQueriesList[y]]:\n",
    "#             avgSim.write(subsetQueriesList[y] + \"@\" + str(sorted(x.items(), key=operator.itemgetter(1))[:50]))\n",
    "\n",
    "    #     print(x)\n",
    "    #     print(average_similarities[subsetQueriesList[42]].get(x))\n",
    "# print(average_similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function that takes a number of expansion words wanted per query word.\n",
    "# def topNIndependentElmo(n):\n",
    "#     with open(\"../Java_API/Car-Grpc/src/main/resources/data/topINELMo/topINEComplete.txt\", \"w\") as ELMoFile:\n",
    "#         outputList = \"\"\n",
    "#         # For each query word, sort these and get the top N words.\n",
    "#         for x in range(len(all_similarities)-1):\n",
    "            \n",
    "#             output = sorted(all_similarities[x].items(), key=lambda kv: kv[1])\n",
    "\n",
    "#             topNWords = [y[0] for y in output[:n+1] if y[0] not in tokens]\n",
    "#             outputList += ' '.join(topNWords) + \"\\n\"\n",
    "            \n",
    "\n",
    "#         ELMoFile.write(outputList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This block generates a map of each unique vocab word to a list of all similarities\n",
    "# # This allows the average of these embeddings to be taken.\n",
    "\n",
    "\n",
    "# queryEmbeddingList = []\n",
    "# querySimilarityList = []\n",
    "\n",
    "# for query in data.get(\"queries\")[:1]: # For each query in the train set\n",
    "#     start = time.time()\n",
    "    \n",
    "#     # Get the formatted Query Id\n",
    "#     queryFormatted = list(set(formatQuery(query.get(\"QueryId\"),False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "    \n",
    "#     # Get the average embedding for each word in the query.\n",
    "#     averageQueryEmbedding = np.average(elmo.embed_sentence(queryFormatted),axis=0)\n",
    "    \n",
    "#     paragraphEmbeddingList = []\n",
    "#     paragraphSimilarity = []\n",
    "    \n",
    "#     for paragraph in query.get(\"Paragraphs\")[:500]: # For each paragraph returned for that query\n",
    "#         sentenceList = []\n",
    "#         sentenceSimilarity = []\n",
    "        \n",
    "#         paragraph = paragraph.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\") # format paragraph\n",
    "#         sentences = list(filter(None,''.join(paragraph).split(\"fullstop\")))\n",
    "\n",
    "#         for x in range(0,len(sentences)): # For each sentence, make it a list and add it back.\n",
    "#             cleanedSentence = sentences[x].strip().split(\" \")\n",
    "#             sentences[x] = cleanedSentence\n",
    "            \n",
    "#         sentencesEmbeddings = elmo.embed_batch(sentences) # Run elmo on the list of sentences in a paragraph\n",
    "\n",
    "#         z = 0\n",
    "        \n",
    "#         for sentenceEmbedding in sentencesEmbeddings: #For each sentence in the paragraph\n",
    "#             sentence = sentences[z]\n",
    "#             y = 0\n",
    "\n",
    "#             perWordEmbedding = np.average(sentenceEmbedding, axis=0) # Average out each word embedding\n",
    "\n",
    "#             for wordEmbed in perWordEmbedding: # For word in sentence\n",
    "#                 paraWordSimilarity = dict()\n",
    "#                 x = 0\n",
    "            \n",
    "#                 paraWordSimilarity[\"sentenceWord\"] = sentence[y] # Store a reference to the word\n",
    "\n",
    "#                 for qWordEmbed in averageQueryEmbedding: # For query word\n",
    "            \n",
    "#                     similarity = round(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed),3) #Get the similarity\n",
    "#                     paraWordSimilarity[queryFormatted[x]] = similarity # Store the similarity mapped to query word\n",
    "#                     x += 1\n",
    "                    \n",
    "#                 querySimilarityList.append(paraWordSimilarity) # Store all similarities for the query\n",
    "#                 y += 1\n",
    "#             z += 1\n",
    "    \n",
    "#     end_time = time.time() - start\n",
    "#     print(end_time)\n",
    "    \n",
    "#     sortedParagraphs = []\n",
    "    \n",
    "#     for k in queryFormatted: # For each query word store the list, sorted the top n by that query word\n",
    "#         testList = []\n",
    "#         testList = sorted(querySimilarityList, key=lambda x: (x[k]))\n",
    "#         sortedParagraphs.append(testList[:10])\n",
    "        \n",
    "# with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/embeddings3.txt\", \"w\") as e:\n",
    "#     e.write(\"{\" + str(sortedParagraphs) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates a list of queries, with a list of paragraphs, with a list of sentences, with a list of words\n",
    "# # Each word has it's similarity measure to each query word. This is useful for looking at the output directly.\n",
    "# # The next block of code is useful for actually using ELMo programmatically.\n",
    "# # This block also contains the actual embedding values stored in a list of lists for each paragraph.\n",
    "\n",
    "\n",
    "# # For each query in the train set\n",
    "# queryEmbeddingList = []\n",
    "# querySimilarityList = []\n",
    "\n",
    "# for query in random.sample(data.get(\"queries\"),500): \n",
    "#     print(query.get(\"QueryId\"))\n",
    "#     start = time.time()\n",
    "    \n",
    "#     # Get the formatted Query Id\n",
    "#     queryFormatted = list(set(formatQuery(query.get(\"QueryId\"),False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "#     print(queryFormatted)\n",
    "#     # Get the average embedding for each word in the query.\n",
    "#     averageQueryEmbedding = np.average(elmo.embed_sentence(queryFormatted),axis=0)\n",
    "    \n",
    "#     paragraphEmbeddingList = []\n",
    "#     paragraphSimilarity = []\n",
    "    \n",
    "#     # For each paragraph returned for that query\n",
    "#     for paragraph in query.get(\"Paragraphs\")[:1]:\n",
    "# #         print(paragraph)\n",
    "#         # format it and give it to the elmo embedder.\n",
    "#         paragraph = paragraph.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "#         test = paragraph.split(\",\")\n",
    "#         sentences = list(filter(None,''.join(test).split(\"fullstop\")))\n",
    "# #         print(sentences)\n",
    "#         for x in range(0,len(sentences)):\n",
    "#             cleanedSentence = sentences[x].strip().split(\" \")\n",
    "#             sentences[x] = cleanedSentence\n",
    "# #         print(len(sentences[0]) + len(sentences[1]))\n",
    "#         # Run elmo on the list of sentences in a paragraph\n",
    "#         sentencesEmbeddings = elmo.embed_batch(sentences)\n",
    "# #         print(len(sentencesEmbeddings[0][0]) + len(sentencesEmbeddings[1][0]))\n",
    "#         sentenceList = []\n",
    "#         sentenceSimilarity = []\n",
    "#         z = 0\n",
    "#         #For each sentence in the paragraph\n",
    "#         for sentenceEmbedding in sentencesEmbeddings:\n",
    "#             sentenceWordSimilarity = []\n",
    "#             sentence = sentences[z]\n",
    "# #             print(sentence)\n",
    "#             # Average out each word embedding\n",
    "#             perWordEmbedding = np.average(sentenceEmbedding, axis=0)\n",
    "            \n",
    "# #             print(\"No. of Similarities:              \" + str(len(perWordEmbedding) * len(averageQueryEmbedding)))\n",
    "#             y = 0\n",
    "    \n",
    "#             # For word in sentence\n",
    "#             for wordEmbed in perWordEmbedding:\n",
    "#                 word = sentence[y]\n",
    "\n",
    "#                 sentenceWordSimilarity.append(word)\n",
    "#                 paraWordSimilarity = dict()\n",
    "#                 x = 0\n",
    "#                 # For query word\n",
    "#                 for qWordEmbed in averageQueryEmbedding:\n",
    "                    \n",
    "#                     #Get the similarity between them\n",
    "#                     paraWordSimilarity[queryFormatted[x]] = round(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed),3)\n",
    "#                     x += 1\n",
    "#                 sentenceWordSimilarity.append(paraWordSimilarity)    \n",
    "#                 y += 1\n",
    "# #             print(\"Number of similarities Generated: \" + str(len(sentenceWordSimilarity) * len(paraWordSimilarity)))\n",
    "#             sentenceSimilarity.append(sentenceWordSimilarity)\n",
    "#             sentenceList.append(perWordEmbedding)\n",
    "#             z += 1\n",
    "\n",
    "#         # Create a list of the sentence embeddings  and similarities\n",
    "#         paragraphEmbeddingList.append(sentenceList)\n",
    "#         paragraphSimilarity.append(sentenceSimilarity)\n",
    "        \n",
    "#     # Create a list of the embeddings and similarities for the paragraphs.\n",
    "#     queryEmbeddingList.append(paragraphEmbeddingList)\n",
    "#     querySimilarityList.append(paragraphSimilarity)\n",
    "#     end_time = time.time() - start\n",
    "#     print(end_time)\n",
    "    \n",
    "# with open(\"../Java_API/Car-Grpc/src/main/resources/data/Local_ELMo/embeddingsView.txt\", \"w\") as e:\n",
    "#     e.write(\"{\" + str(querySimilarityList) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in data.get(\"queries\")[:10]: \n",
    "#     queryFormatted = list(set(formatQuery(query.get(\"QueryId\"),False).replace(\"enwiki:\",\"\").split(\" \")))\n",
    "#     averageQueryEmbedding = np.average(elmo.embed_sentence(queryFormatted),axis=0)\n",
    "#     print(averageQueryEmbedding.shape)\n",
    "#     for paragraph in query.get(\"Paragraphs\")[:10]:\n",
    "#         paragraph = paragraph.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        \n",
    "#         test = ' '.join(paragraph.split(\",\"))\n",
    "#         sentences = test.split(\"fullstop\")\n",
    "#         for x in range(0,len(sentences)):\n",
    "#             cleanedSentence = sentences[x].replace(\"  \",\" \").strip().split(\" \")\n",
    "#             sentences[x] = cleanedSentence\n",
    "#             # Input query as a list\n",
    "#         sentencesEmbeddings = elmo.embed_batch(sentences)\n",
    "#         for sentenceEmbedding in sentencesEmbeddings:\n",
    "#             perWordEmbedding = np.average(sentenceEmbedding, axis=0)\n",
    "#             similarity = dict()\n",
    "#             for wordEmbed in perWordEmbedding:\n",
    "#                 for qWordEmbed in averageQueryEmbedding:\n",
    "#                     similarity[qWordEmbed]\n",
    "#                     print(scipy.spatial.distance.cosine(wordEmbed,qWordEmbed))\n",
    "# #         # Calculate the average embedding for each word\n",
    "# #         for x in range(len(tokens)):\n",
    "# #             avg_tokens.append((vectors[0][x] + vectors[1][x] + vectors[2][x])/3)\n",
    "# #     print((x).get(\"QueryId\"))\n",
    "\n",
    "# # print(data.get(\"queries\")[0].get(\"QueryId\"))\n",
    "# # print(data.get(\"queries\")[0].get(\"Paragraphs\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
